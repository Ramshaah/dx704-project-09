{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 9 Project\n",
        "\n",
        "This week's project will build an email spam classifier based on the Enron email data set.\n",
        "You will perform your own feature extraction, and use naive Bayes to estimate the probability that a particular email is spam or not.\n",
        "Finally, you will review the tradeoffs from different thresholds for automatically sending emails to the junk folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBdILvlviZs2"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 9 Materials](https://github.com/bu-cds-dx704/dx704-project-09).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDlXZBVd2aR"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1nEl0_Khm_"
      },
      "source": [
        "## Part 1: Download Data Set\n",
        "\n",
        "We will be using the Enron spam data set as prepared in this GitHub repository.\n",
        "\n",
        "https://github.com/MWiechmann/enron_spam_data\n",
        "\n",
        "You may need to download this differently depending on your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports used throughout the notebook\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwpoSzUxKmG9",
        "outputId": "3ace62f1-c32a-462a-d538-36f3638b7dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-10-31 22:29:17--  https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
            "Resolving github.com (github.com)... 20.207.73.82\n",
            "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip [following]\n",
            "--2025-10-31 22:29:18--  https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15642124 (15M) [application/zip]\n",
            "Saving to: ‘enron_spam_data.zip.2’\n",
            "\n",
            "enron_spam_data.zip 100%[===================>]  14.92M  23.7MB/s    in 0.6s    \n",
            "\n",
            "2025-10-31 22:29:19 (23.7 MB/s) - ‘enron_spam_data.zip.2’ saved [15642124/15642124]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EfCir3ILLv8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "9gn-4hUzLywO",
        "outputId": "f810652e-7829-44dd-e842-137281f53be2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>meter 7268 nov allocation</td>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33711</th>\n",
              "      <td>33711</td>\n",
              "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
              "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33712</th>\n",
              "      <td>33712</td>\n",
              "      <td>all prescript medicines are on special . to be...</td>\n",
              "      <td>i got it earlier than expected and it was wrap...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33713</th>\n",
              "      <td>33713</td>\n",
              "      <td>the next generation online pharmacy .</td>\n",
              "      <td>are you ready to rock on ? let the man in you ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33714</th>\n",
              "      <td>33714</td>\n",
              "      <td>bloow in 5 - 10 times the time</td>\n",
              "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33715</th>\n",
              "      <td>33715</td>\n",
              "      <td>dear sir , i am interested in it</td>\n",
              "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33716 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Message ID                                            Subject  \\\n",
              "0               0                       christmas tree farm pictures   \n",
              "1               1                           vastar resources , inc .   \n",
              "2               2                       calpine daily gas nomination   \n",
              "3               3                                         re : issue   \n",
              "4               4                          meter 7268 nov allocation   \n",
              "...           ...                                                ...   \n",
              "33711       33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
              "33712       33712  all prescript medicines are on special . to be...   \n",
              "33713       33713              the next generation online pharmacy .   \n",
              "33714       33714                     bloow in 5 - 10 times the time   \n",
              "33715       33715                   dear sir , i am interested in it   \n",
              "\n",
              "                                                 Message Spam/Ham        Date  \n",
              "0                                                    NaN      ham  1999-12-10  \n",
              "1      gary , production from the high island larger ...      ham  1999-12-13  \n",
              "2                 - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
              "3      fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
              "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
              "...                                                  ...      ...         ...  \n",
              "33711  hello , welcome to gigapharm onlinne shop .\\np...     spam  2005-07-29  \n",
              "33712  i got it earlier than expected and it was wrap...     spam  2005-07-29  \n",
              "33713  are you ready to rock on ? let the man in you ...     spam  2005-07-30  \n",
              "33714  learn how to last 5 - 10 times longer in\\nbed ...     spam  2005-07-30  \n",
              "33715  hi : )\\ndo you need some softwares ? i can giv...     spam  2005-07-31  \n",
              "\n",
              "[33716 rows x 5 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pandas can read the zip file directly\n",
        "enron_spam_data = pd.read_csv(\"enron_spam_data.zip\")\n",
        "enron_spam_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYypb_fJWF_A",
        "outputId": "17478b00-1c10-4026-a42b-dcc8a368eab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5092834262664611)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(enron_spam_data[\"Spam/Ham\"] == \"spam\").mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 2: Design a Feature Extractor\n",
        "\n",
        "Design a feature extractor for this data set and write out two files of features based on the text.\n",
        "Don't forget that both the Subject and Message columns are relevant sources of text data.\n",
        "For each email, you should count the number of repetitions of each feature present.\n",
        "The auto-grader will assume that you are using a multinomial distribution in the following problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-CF6wtn_VRjp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train-features.tsv (32592, 2)\n",
            "Saved test-features.tsv (1124, 2)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd\n",
        "import re, json\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset (as in Part 1)\n",
        "# !wget https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
        "df = pd.read_csv(\"enron_spam_data.zip\")  # columns: Message ID, Subject, Message, Spam/Ham\n",
        "\n",
        "# ---------- Tokenization & Normalization ----------\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    # canonicalize urls and numbers to reduce vocab explosion\n",
        "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" <url> \", s)\n",
        "    s = re.sub(r\"\\d+\", \" <num> \", s)\n",
        "    return s\n",
        "\n",
        "def tokenize(s: str):\n",
        "    # keep <url> and <num> as tokens, and capture alnum/underscore tokens\n",
        "    return re.findall(r\"<url>|<num>|[a-z0-9_]+\", s)\n",
        "\n",
        "# ---------- Feature Extractor ----------\n",
        "# Multinomial-friendly integer counts; subject lightly up-weighted\n",
        "\n",
        "def extract_features(subject: str, message: str) -> Counter:\n",
        "    subj = normalize_text(subject)\n",
        "    body = normalize_text(message)\n",
        "\n",
        "    toks_subj = tokenize(subj)\n",
        "    toks_body = tokenize(body)\n",
        "\n",
        "    # upweight subject: duplicate once (simple integer upweight)\n",
        "    toks = toks_body + toks_subj + toks_subj\n",
        "\n",
        "    feats = Counter()\n",
        "\n",
        "    # Unigrams (prefix to avoid collisions)\n",
        "    for t in toks:\n",
        "        feats[f\"uni::{t}\"] += 1\n",
        "\n",
        "    # Light bigrams (only from body to keep vocab modest)\n",
        "    for i in range(len(toks_body) - 1):\n",
        "        b = f\"{toks_body[i]} {toks_body[i+1]}\"\n",
        "        feats[f\"bi::{b}\"] += 1\n",
        "\n",
        "    # Structural hints often correlated with spam\n",
        "    num_urls = sum(1 for t in toks if t == \"<url>\")\n",
        "    feats[\"meta::num_urls\"] = num_urls\n",
        "    feats[\"meta::has_url\"] = 1 if num_urls > 0 else 0\n",
        "    feats[\"meta::len\"] = max(1, len(toks_body))          # length proxy (int)\n",
        "    feats[\"meta::subject_len\"] = max(1, len(toks_subj))  # subject length (int)\n",
        "\n",
        "    return feats\n",
        "\n",
        "# ---------- Train/Test Split ----------\n",
        "# Test if Message ID % 30 == 0, else Train\n",
        "\n",
        "def is_test_row(message_id) -> bool:\n",
        "    try:\n",
        "        return int(message_id) % 30 == 0\n",
        "    except:\n",
        "        # fallback (rare) if non-integer IDs appear\n",
        "        return (hash(str(message_id)) % 30) == 0\n",
        "\n",
        "rows = []\n",
        "for _, r in df.iterrows():\n",
        "    feats = extract_features(r.get(\"Subject\", \"\"), r.get(\"Message\", \"\"))\n",
        "    rows.append({\n",
        "        \"Message ID\": r[\"Message ID\"],\n",
        "        # compact JSON; values are integers\n",
        "        \"features_json\": json.dumps({k:int(v) for k,v in feats.items()}, separators=(',', ':'))\n",
        "    })\n",
        "\n",
        "feat_df = pd.DataFrame(rows)\n",
        "train_out = feat_df[~feat_df[\"Message ID\"].apply(is_test_row)][[\"Message ID\", \"features_json\"]]\n",
        "test_out  = feat_df[ feat_df[\"Message ID\"].apply(is_test_row)][[\"Message ID\", \"features_json\"]]\n",
        "\n",
        "train_out.to_csv(\"train-features.tsv\", sep=\"\\t\", index=False)\n",
        "test_out.to_csv(\"test-features.tsv\",  sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved train-features.tsv\", train_out.shape)\n",
        "print(\"Saved test-features.tsv\",  test_out.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90ug-qYVWI2"
      },
      "source": [
        "Assign a row to the test data set if `Message ID % 30 == 0` and assign it to the training data set otherwise.\n",
        "Write two files, \"train-features.tsv\" and \"test-features.tsv\" with two columns, Message ID and features_json.\n",
        "The features_json column should contain a JSON dictionary where the keys are your feature names and the values are integer feature values.\n",
        "This will give us a sparse feature representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t7AjXVlXUpaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train rows: 32592 Test rows: 1124\n",
            "Avg features/email — train: 310.4 test: 291.44\n",
            "Sample train JSON: {\"uni::gary\":2,\"uni::production\":3,\"uni::from\":3,\"uni::the\":8,\"uni::high\":2,\"uni::island\":2,\"uni::larger\":2,\"uni::block\":2,\"uni::a\":6,\"uni::<num>\":149,\"uni::commenced\":1,\"uni::on\":7,\"uni::saturday\":1, ...\n",
            "Sample test  JSON: {\"uni::christmas\":2,\"uni::tree\":2,\"uni::farm\":2,\"uni::pictures\":2,\"meta::num_urls\":0,\"meta::has_url\":0,\"meta::len\":1,\"meta::subject_len\":4} ...\n",
            "✅ Part 2 format & split look good.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import pandas as pd, json, numpy as np\n",
        "\n",
        "def load_tsv(path):\n",
        "    df = pd.read_csv(path, sep=\"\\t\", dtype={\"Message ID\": object, \"features_json\": str})\n",
        "    assert list(df.columns) == [\"Message ID\",\"features_json\"], f\"Bad columns in {path}: {df.columns}\"\n",
        "    return df\n",
        "\n",
        "train = load_tsv(\"train-features.tsv\")\n",
        "test  = load_tsv(\"test-features.tsv\")\n",
        "\n",
        "# 1) Message ID must be convertible to int and split rule must hold\n",
        "def to_int_mid(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        raise AssertionError(f\"Message ID not int-like: {x!r}\")\n",
        "\n",
        "train[\"MID\"] = train[\"Message ID\"].map(to_int_mid)\n",
        "test[\"MID\"]  = test[\"Message ID\"].map(to_int_mid)\n",
        "\n",
        "assert ((test[\"MID\"] % 30) == 0).all(), \"Some test rows are not MID % 30 == 0\"\n",
        "assert ((train[\"MID\"] % 30) != 0).all(), \"Some train rows violate MID % 30 != 0\"\n",
        "\n",
        "# 2) features_json must be valid JSON dict with integer values\n",
        "def check_json(row):\n",
        "    d = json.loads(row)\n",
        "    assert isinstance(d, dict), \"features_json must be a JSON object\"\n",
        "    for k,v in d.items():\n",
        "        assert isinstance(k, str), \"feature names must be strings\"\n",
        "        assert isinstance(v, (int, np.integer)), f\"value for {k} must be integer, got {type(v)}\"\n",
        "    return len(d)\n",
        "\n",
        "train_nonzero = train[\"features_json\"].map(check_json)\n",
        "test_nonzero  = test[\"features_json\"].map(check_json)\n",
        "\n",
        "# 3) No empty dicts (every email should yield at least one feature)\n",
        "assert (train_nonzero > 0).all(), \"Empty feature dict in train\"\n",
        "assert (test_nonzero  > 0).all(), \"Empty feature dict in test\"\n",
        "\n",
        "# 4) Quick sparsity + sanity peek\n",
        "print(\"Train rows:\", len(train), \"Test rows:\", len(test))\n",
        "print(\"Avg features/email — train:\", round(train_nonzero.mean(),2), \"test:\", round(test_nonzero.mean(),2))\n",
        "\n",
        "# 5) Optional: spot-check a couple of JSONs\n",
        "print(\"Sample train JSON:\", train.loc[train.index[0], \"features_json\"][:200], \"...\")\n",
        "print(\"Sample test  JSON:\",  test.loc[test.index[0],  \"features_json\"][:200], \"...\")\n",
        "print(\"✅ Part 2 format & split look good.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEYBd7WUrC0"
      },
      "source": [
        "Submit \"train-features.tsv\" and \"test-features.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrLU0aIaNB7"
      },
      "source": [
        "Hint: these features will be graded based on the test accuracy of a logistic regression based on the training features.\n",
        "This is to make sure that your feature set is not degenerate; you do not need to compute this regression yourself.\n",
        "You can separately assess your feature quality based on your results in part 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PhU4d5vEFX"
      },
      "source": [
        "## Part 3: Compute Conditional Probabilities\n",
        "\n",
        "Based on your training data, compute appropriate conditional probabilities for use with naïve Bayes.\n",
        "Use of additive smoothing with $\\alpha=1$ to avoid zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3MKi6er-Vde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 1,443,095 | Ham tokens: 13,266,275 | Spam tokens: 10,514,414\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>ham_probability</th>\n",
              "      <th>spam_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bi::&lt;num&gt; &lt;num&gt;</td>\n",
              "      <td>1.042254e-02</td>\n",
              "      <td>5.332549e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bi::&lt;num&gt; _</td>\n",
              "      <td>2.875718e-05</td>\n",
              "      <td>4.373821e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bi::&lt;num&gt; a</td>\n",
              "      <td>1.035394e-04</td>\n",
              "      <td>1.933513e-04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bi::&lt;num&gt; aa</td>\n",
              "      <td>6.798388e-07</td>\n",
              "      <td>6.523098e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bi::&lt;num&gt; aaa</td>\n",
              "      <td>2.719355e-07</td>\n",
              "      <td>8.362946e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>bi::&lt;num&gt; aabda</td>\n",
              "      <td>1.359678e-07</td>\n",
              "      <td>8.362946e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>bi::&lt;num&gt; aads</td>\n",
              "      <td>6.798388e-08</td>\n",
              "      <td>3.345178e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>bi::&lt;num&gt; aakkyl</td>\n",
              "      <td>6.798388e-08</td>\n",
              "      <td>1.672589e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>bi::&lt;num&gt; aalman</td>\n",
              "      <td>6.798388e-08</td>\n",
              "      <td>1.672589e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>bi::&lt;num&gt; aambique</td>\n",
              "      <td>6.798388e-08</td>\n",
              "      <td>1.672589e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              feature  ham_probability  spam_probability\n",
              "0     bi::<num> <num>     1.042254e-02      5.332549e-03\n",
              "1         bi::<num> _     2.875718e-05      4.373821e-05\n",
              "2         bi::<num> a     1.035394e-04      1.933513e-04\n",
              "3        bi::<num> aa     6.798388e-07      6.523098e-06\n",
              "4       bi::<num> aaa     2.719355e-07      8.362946e-08\n",
              "5     bi::<num> aabda     1.359678e-07      8.362946e-08\n",
              "6      bi::<num> aads     6.798388e-08      3.345178e-07\n",
              "7    bi::<num> aakkyl     6.798388e-08      1.672589e-07\n",
              "8    bi::<num> aalman     6.798388e-08      1.672589e-07\n",
              "9  bi::<num> aambique     6.798388e-08      1.672589e-07"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import pandas as pd, json, numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "ALPHA = 1.0  # Laplace smoothing α=1\n",
        "\n",
        "# 1) Load features (train only) and labels\n",
        "train_feat = pd.read_csv(\"train-features.tsv\", sep=\"\\t\", dtype={\"Message ID\": int, \"features_json\": str})\n",
        "raw = pd.read_csv(\"enron_spam_data.zip\")  # from Part 1\n",
        "raw = raw.rename(columns={\"Message ID\":\"Message ID\"})  # ensure same name\n",
        "labels = raw.loc[:, [\"Message ID\", \"Spam/Ham\"]].drop_duplicates()\n",
        "\n",
        "train = train_feat.merge(labels, on=\"Message ID\", how=\"left\")\n",
        "assert train[\"Spam/Ham\"].notna().all(), \"Some training Message IDs missing labels—double-check IDs & dataset.\"\n",
        "\n",
        "# 2) Build per-class token counts (multinomial)\n",
        "#    We'll accumulate counts for each feature, separately for ham and spam\n",
        "class_feature_counts = {\"ham\": Counter(), \"spam\": Counter()}\n",
        "class_token_totals  = {\"ham\": 0, \"spam\": 0}\n",
        "vocab = set()\n",
        "\n",
        "for _, row in train.iterrows():\n",
        "    y = row[\"Spam/Ham\"].strip().lower()  # 'ham' or 'spam'\n",
        "    feats = json.loads(row[\"features_json\"])\n",
        "    # ensure integer counts\n",
        "    feats = {str(k): int(v) for k, v in feats.items() if int(v) != 0}\n",
        "    vocab.update(feats.keys())\n",
        "    class_feature_counts[y].update(feats)\n",
        "    class_token_totals[y] += sum(feats.values())\n",
        "\n",
        "V = len(vocab)\n",
        "N_ham  = class_token_totals[\"ham\"]\n",
        "N_spam = class_token_totals[\"spam\"]\n",
        "\n",
        "print(f\"Vocab size: {V:,} | Ham tokens: {N_ham:,} | Spam tokens: {N_spam:,}\")\n",
        "\n",
        "# 3) Compute smoothed conditional probabilities for every feature in vocab\n",
        "#    P(f|class) = (count(f,class) + α) / (total_tokens_class + α * V)\n",
        "den_ham  = N_ham  + ALPHA * V\n",
        "den_spam = N_spam + ALPHA * V\n",
        "\n",
        "records = []\n",
        "get_hc = class_feature_counts[\"ham\"].get\n",
        "get_sc = class_feature_counts[\"spam\"].get\n",
        "\n",
        "for f in vocab:\n",
        "    c_ham  = get_hc(f, 0)\n",
        "    c_spam = get_sc(f, 0)\n",
        "    p_ham  = (c_ham  + ALPHA) / den_ham\n",
        "    p_spam = (c_spam + ALPHA) / den_spam\n",
        "    records.append((f, p_ham, p_spam))\n",
        "\n",
        "feature_probs = pd.DataFrame(records, columns=[\"feature\",\"ham_probability\",\"spam_probability\"])\\\n",
        "                    .sort_values(\"feature\").reset_index(drop=True)\n",
        "\n",
        "# 4) Save file\n",
        "feature_probs.to_csv(\"feature-probabilities.tsv\", sep=\"\\t\", index=False)\n",
        "feature_probs.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbDJfLCdVfHh"
      },
      "source": [
        "Save the conditional probabilities in a file \"feature-probabilities.tsv\" with columns feature, ham_probability and spam_probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kTVFW327VsOC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1443095, 3) ['feature', 'ham_probability', 'spam_probability']\n",
            "           feature  ham_probability  spam_probability\n",
            "0  bi::<num> <num>     1.042254e-02      5.332549e-03\n",
            "1      bi::<num> _     2.875718e-05      4.373821e-05\n",
            "2      bi::<num> a     1.035394e-04      1.933513e-04\n",
            "3     bi::<num> aa     6.798388e-07      6.523098e-06\n",
            "4    bi::<num> aaa     2.719355e-07      8.362946e-08\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# feature_probs must have these exact columns:\n",
        "# [\"feature\", \"ham_probability\", \"spam_probability\"]\n",
        "\n",
        "assert list(feature_probs.columns) == [\"feature\",\"ham_probability\",\"spam_probability\"]\n",
        "\n",
        "# Save as TSV (no index)\n",
        "feature_probs.to_csv(\"feature-probabilities.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# Quick sanity check\n",
        "check = pd.read_csv(\"feature-probabilities.tsv\", sep=\"\\t\")\n",
        "print(check.shape, check.columns.tolist())\n",
        "print(check.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-k6K-hVt6q"
      },
      "source": [
        "Submit \"feature-probabilities.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQpZbILYqNd"
      },
      "source": [
        "## Part 4: Implement a Naïve Bayes Classifier\n",
        "\n",
        "Implement a naïve Bayes classifier based on your previous feature probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jkZeyZgsWr5-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Priors — P(spam)=0.5093, P(ham)=0.4907\n",
            "Saved train-predictions.tsv (32592, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>ham</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.066832e-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.731193e-98</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Message ID  ham          spam\n",
              "0           1  1.0  0.000000e+00\n",
              "1           2  1.0  3.066832e-26\n",
              "2           3  1.0  0.000000e+00\n",
              "3           4  1.0  0.000000e+00\n",
              "4           5  1.0  1.731193e-98"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# === Part 4: Multinomial Naïve Bayes classifier for TRAIN set ===\n",
        "import json, math\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1) Load inputs\n",
        "train_feats = pd.read_csv(\"train-features.tsv\", sep=\"\\t\")          # [Message ID, features_json]\n",
        "feat_probs  = pd.read_csv(\"feature-probabilities.tsv\", sep=\"\\t\")   # [feature, ham_probability, spam_probability]\n",
        "\n",
        "# We need labels to get class priors from TRAIN split only\n",
        "enron = pd.read_csv(\"enron_spam_data.zip\")\n",
        "labels = enron[[\"Message ID\",\"Spam/Ham\"]].copy()\n",
        "labels[\"is_spam\"] = (labels[\"Spam/Ham\"].str.lower() == \"spam\").astype(int)\n",
        "labels_train = labels[labels[\"Message ID\"] % 30 != 0].copy()\n",
        "\n",
        "# 2) Class priors P(class)\n",
        "n_spam = labels_train[\"is_spam\"].sum()\n",
        "n_ham  = len(labels_train) - n_spam\n",
        "p_spam = n_spam / len(labels_train)\n",
        "p_ham  = n_ham  / len(labels_train)\n",
        "\n",
        "log_p_spam = math.log(p_spam) if p_spam > 0 else -1e12\n",
        "log_p_ham  = math.log(p_ham)  if p_ham  > 0 else -1e12\n",
        "\n",
        "print(f\"Priors — P(spam)={p_spam:.4f}, P(ham)={p_ham:.4f}\")\n",
        "\n",
        "# 3) Fast lookups for P(f|class); use safe floors for truly unseen keys\n",
        "p_f_ham  = dict(zip(feat_probs[\"feature\"], feat_probs[\"ham_probability\"]))\n",
        "p_f_spam = dict(zip(feat_probs[\"feature\"], feat_probs[\"spam_probability\"]))\n",
        "\n",
        "# Floors: small but nonzero (your Part 3 used Laplace; these only catch features that somehow\n",
        "# appear in features_json but not in the saved table due to edge cases)\n",
        "floor_ham  = max(min((v for v in p_f_ham.values()  if v > 0), default=1e-12) * 0.1, 1e-12)\n",
        "floor_spam = max(min((v for v in p_f_spam.values() if v > 0), default=1e-12) * 0.1, 1e-12)\n",
        "\n",
        "log_p_f_ham  = defaultdict(lambda: math.log(floor_ham),\n",
        "                           {f: math.log(max(p, 1e-300)) for f,p in p_f_ham.items()})\n",
        "log_p_f_spam = defaultdict(lambda: math.log(floor_spam),\n",
        "                           {f: math.log(max(p, 1e-300)) for f,p in p_f_spam.items()})\n",
        "\n",
        "# 4) Scoring: log P(class) + sum_f count(f) * log P(f|class), then normalize\n",
        "def score_features_json(features_json: str):\n",
        "    feats = json.loads(features_json)\n",
        "    ll_spam = log_p_spam\n",
        "    ll_ham  = log_p_ham\n",
        "    for f, c in feats.items():\n",
        "        if not c:\n",
        "            continue\n",
        "        ll_spam += c * log_p_f_spam[f]\n",
        "        ll_ham  += c * log_p_f_ham[f]\n",
        "    # log-sum-exp to get probabilities\n",
        "    m = max(ll_spam, ll_ham)\n",
        "    ex_spam = math.exp(ll_spam - m)\n",
        "    ex_ham  = math.exp(ll_ham  - m)\n",
        "    denom = ex_spam + ex_ham\n",
        "    return ex_ham/denom, ex_spam/denom  # (ham, spam)\n",
        "\n",
        "# 5) Score every TRAIN row and save\n",
        "pred_rows = []\n",
        "for mid, fj in zip(train_feats[\"Message ID\"], train_feats[\"features_json\"]):\n",
        "    p_ham_i, p_spam_i = score_features_json(fj)\n",
        "    pred_rows.append((mid, p_ham_i, p_spam_i))\n",
        "\n",
        "train_preds = pd.DataFrame(pred_rows, columns=[\"Message ID\",\"ham\",\"spam\"])\n",
        "train_preds.to_csv(\"train-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved train-predictions.tsv\", train_preds.shape)\n",
        "display(train_preds.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYGfCYXW89l"
      },
      "source": [
        "Save your prediction probabilities to \"train-predictions.tsv\" with columns Message ID, ham and spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kCKrHbpqZ1gY"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGHYjWN9Z3Sq"
      },
      "source": [
        "Submit \"train-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTlyFLDOCDj"
      },
      "source": [
        "## Part 5: Predict Spam Probability for Test Data\n",
        "\n",
        "Use your previous classifier to predict spam probability for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UELHs9CzXaz1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved test-predictions.tsv (1124, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>ham</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>9.990951e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.142316e-222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.771037e-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>90</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.464752e-69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>120</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Message ID       ham           spam\n",
              "0           0  0.000905   9.990951e-01\n",
              "1          30  1.000000  6.142316e-222\n",
              "2          60  1.000000   1.771037e-26\n",
              "3          90  1.000000   3.464752e-69\n",
              "4         120  1.000000   0.000000e+00"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# === Part 5: Predict spam probability for TEST set ===\n",
        "import json, math\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# 1) Load inputs\n",
        "test_feats  = pd.read_csv(\"test-features.tsv\", sep=\"\\t\")             # [Message ID, features_json]\n",
        "feat_probs  = pd.read_csv(\"feature-probabilities.tsv\", sep=\"\\t\")     # [feature, ham_probability, spam_probability]\n",
        "\n",
        "# Priors from TRAIN split only (same as Part 4 to keep consistency)\n",
        "enron = pd.read_csv(\"enron_spam_data.zip\")\n",
        "labels = enron[[\"Message ID\",\"Spam/Ham\"]].copy()\n",
        "labels[\"is_spam\"] = (labels[\"Spam/Ham\"].str.lower() == \"spam\").astype(int)\n",
        "labels_train = labels[labels[\"Message ID\"] % 30 != 0].copy()\n",
        "\n",
        "n_spam = labels_train[\"is_spam\"].sum()\n",
        "n_ham  = len(labels_train) - n_spam\n",
        "p_spam = n_spam / len(labels_train)\n",
        "p_ham  = n_ham  / len(labels_train)\n",
        "\n",
        "log_p_spam = math.log(p_spam) if p_spam > 0 else -1e12\n",
        "log_p_ham  = math.log(p_ham)  if p_ham  > 0 else -1e12\n",
        "\n",
        "# 2) Fast lookups for P(f|class) from Part 3 output\n",
        "p_f_ham  = dict(zip(feat_probs[\"feature\"], feat_probs[\"ham_probability\"]))\n",
        "p_f_spam = dict(zip(feat_probs[\"feature\"], feat_probs[\"spam_probability\"]))\n",
        "\n",
        "# Floors for any rare/unseen features\n",
        "floor_ham  = max(min((v for v in p_f_ham.values()  if v > 0), default=1e-12) * 0.1, 1e-12)\n",
        "floor_spam = max(min((v for v in p_f_spam.values() if v > 0), default=1e-12) * 0.1, 1e-12)\n",
        "\n",
        "log_p_f_ham  = defaultdict(lambda: math.log(floor_ham),\n",
        "                           {f: math.log(max(p, 1e-300)) for f,p in p_f_ham.items()})\n",
        "log_p_f_spam = defaultdict(lambda: math.log(floor_spam),\n",
        "                           {f: math.log(max(p, 1e-300)) for f,p in p_f_spam.items()})\n",
        "\n",
        "def score_features_json(features_json: str):\n",
        "    feats = json.loads(features_json)\n",
        "    ll_spam = log_p_spam\n",
        "    ll_ham  = log_p_ham\n",
        "    for f, c in feats.items():\n",
        "        if not c:\n",
        "            continue\n",
        "        ll_spam += c * log_p_f_spam[f]\n",
        "        ll_ham  += c * log_p_f_ham[f]\n",
        "    m = max(ll_spam, ll_ham)\n",
        "    ex_spam = math.exp(ll_spam - m)\n",
        "    ex_ham  = math.exp(ll_ham  - m)\n",
        "    denom = ex_spam + ex_ham\n",
        "    return ex_ham/denom, ex_spam/denom  # (ham, spam)\n",
        "\n",
        "# 3) Score TEST and save\n",
        "pred_rows = []\n",
        "for mid, fj in zip(test_feats[\"Message ID\"], test_feats[\"features_json\"]):\n",
        "    p_ham_i, p_spam_i = score_features_json(fj)\n",
        "    pred_rows.append((mid, p_ham_i, p_spam_i))\n",
        "\n",
        "test_preds = pd.DataFrame(pred_rows, columns=[\"Message ID\",\"ham\",\"spam\"])\n",
        "test_preds.to_csv(\"test-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Saved test-predictions.tsv\", test_preds.shape)\n",
        "display(test_preds.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opc86JSEaAQM"
      },
      "source": [
        "Save your prediction probabilities in \"test-predictions.tsv\" with the same columns as \"train-predictions.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qIg1XaY_Z_Rr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLbyE8paGqM"
      },
      "source": [
        "Submit \"test-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6ReUMsZNZ8"
      },
      "source": [
        "## Part 6: Construct ROC Curve\n",
        "\n",
        "For every probability threshold from 0.01 to .99 in increments of 0.01, compute the false and true positive rates from the test data using the spam class for positives.\n",
        "That is, if the predicted spam probability is greater than or equal to the threshold, predict spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QAx9jbDBYOVo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved roc.tsv (99, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threshold</th>\n",
              "      <th>false_positive_rate</th>\n",
              "      <th>true_positive_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>0.996503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>0.994755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>0.994755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>0.994755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.012681</td>\n",
              "      <td>0.994755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   threshold  false_positive_rate  true_positive_rate\n",
              "0       0.01             0.012681            0.996503\n",
              "1       0.02             0.012681            0.994755\n",
              "2       0.03             0.012681            0.994755\n",
              "3       0.04             0.012681            0.994755\n",
              "4       0.05             0.012681            0.994755"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# === Part 6: ROC curve from test predictions ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load test labels and predictions\n",
        "enron = pd.read_csv(\"enron_spam_data.zip\")  # has \"Spam/Ham\" and \"Message ID\"\n",
        "test_labels = (\n",
        "    enron[enron[\"Message ID\"] % 30 == 0]\n",
        "    .loc[:, [\"Message ID\", \"Spam/Ham\"]]\n",
        "    .assign(is_spam=lambda df: (df[\"Spam/Ham\"].str.lower() == \"spam\").astype(int))\n",
        ")\n",
        "\n",
        "test_preds = pd.read_csv(\"test-predictions.tsv\", sep=\"\\t\")  # Message ID, ham, spam\n",
        "\n",
        "# 2) Merge to align rows\n",
        "df = test_labels.merge(test_preds, on=\"Message ID\", how=\"inner\")\n",
        "\n",
        "y_true = df[\"is_spam\"].to_numpy()\n",
        "p_spam = df[\"spam\"].to_numpy()\n",
        "\n",
        "# 3) Compute FPR/TPR for thresholds 0.01..0.99\n",
        "rows = []\n",
        "thresholds = np.round(np.arange(0.01, 1.00, 0.01), 2)\n",
        "\n",
        "P = (y_true == 1).sum()  # positives (spam)\n",
        "N = (y_true == 0).sum()  # negatives (ham)\n",
        "\n",
        "for t in thresholds:\n",
        "    y_hat = (p_spam >= t).astype(int)\n",
        "    TP = int(((y_hat == 1) & (y_true == 1)).sum())\n",
        "    FP = int(((y_hat == 1) & (y_true == 0)).sum())\n",
        "    TN = int(((y_hat == 0) & (y_true == 0)).sum())\n",
        "    FN = int(((y_hat == 0) & (y_true == 1)).sum())\n",
        "\n",
        "    tpr = TP / P if P else 0.0  # True Positive Rate (Recall)\n",
        "    fpr = FP / N if N else 0.0  # False Positive Rate\n",
        "\n",
        "    rows.append((float(t), fpr, tpr))\n",
        "\n",
        "roc = pd.DataFrame(rows, columns=[\"threshold\", \"false_positive_rate\", \"true_positive_rate\"])\n",
        "\n",
        "# 4) Save\n",
        "roc.to_csv(\"roc.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved roc.tsv\", roc.shape)\n",
        "roc.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGaDOauX2vE"
      },
      "source": [
        "Save this data in a file \"roc.tsv\" with columns threshold, false_positive_rate and true_positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eSHCzA85YP_I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Approx AUC (trapezoid over FPR–TPR): 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_21935/2979854228.py:4: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  auc = np.trapz(roc[\"true_positive_rate\"].to_numpy(), roc[\"false_positive_rate\"].to_numpy())\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Optional sanity check (trapezoidal AUC):\n",
        "auc = np.trapz(roc[\"true_positive_rate\"].to_numpy(), roc[\"false_positive_rate\"].to_numpy())\n",
        "print(\"Approx AUC (trapezoid over FPR–TPR):\", round(float(auc), 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4po8_NMYRuo"
      },
      "source": [
        "Submit \"roc.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynaBbiCZhMYi"
      },
      "source": [
        "## Part 7: Signup for Gemini API Key\n",
        "\n",
        "Create a free Gemini API key at https://aistudio.google.com/app/api-keys.\n",
        "You will need to do this with a personal Google account - it will not work with your BU Google account.\n",
        "This will not incur any charges unless you configure billing information for the key.\n",
        "\n",
        "You will be asked to start a Gemini free trial for week 11.\n",
        "This will not incur any charges unless you exceed expected usage by an order of magnitude.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3xFKcX6hxTL"
      },
      "source": [
        "No submission needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote files:\n",
            " - train-features.tsv\n",
            " - test-features.tsv\n",
            " - feature-probabilities.tsv\n",
            " - train-predictions.tsv\n",
            " - test-predictions.tsv\n",
            " - roc.tsv\n"
          ]
        }
      ],
      "source": [
        "# === Week 9: Rebuild all artifacts for Gradescope ===\n",
        "import re, json, math\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -------- Part 1: Data --------\n",
        "!wget -q -O enron_spam_data.zip https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
        "df_raw = pd.read_csv(\"enron_spam_data.zip\")  # has columns: Message ID, Subject, Message, Spam/Ham\n",
        "\n",
        "# -------- Helpers --------\n",
        "def normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" <url> \", s)\n",
        "    s = re.sub(r\"\\d+\", \" <num> \", s)\n",
        "    return s\n",
        "\n",
        "def tokenize(s: str):\n",
        "    return re.findall(r\"<url>|<num>|[a-z0-9_]+\", s)\n",
        "\n",
        "def extract_features(subject: str, message: str) -> Counter:\n",
        "    subj = normalize_text(subject)\n",
        "    body = normalize_text(message)\n",
        "    toks_subj = tokenize(subj)\n",
        "    toks_body = tokenize(body)\n",
        "    toks = toks_body + toks_subj + toks_subj  # upweight subject\n",
        "    feats = Counter()\n",
        "    for t in toks:\n",
        "        feats[f\"uni::{t}\"] += 1\n",
        "    for i in range(len(toks_body) - 1):\n",
        "        feats[f\"bi::{toks_body[i]} {toks_body[i+1]}\"] += 1\n",
        "    num_urls = sum(1 for t in toks if t == \"<url>\")\n",
        "    feats[\"meta::num_urls\"] = num_urls\n",
        "    feats[\"meta::has_url\"] = 1 if num_urls > 0 else 0\n",
        "    feats[\"meta::len\"] = max(1, len(toks_body))\n",
        "    feats[\"meta::subject_len\"] = max(1, len(toks_subj))\n",
        "    return feats\n",
        "\n",
        "def is_test(message_id) -> bool:\n",
        "    return int(message_id) % 30 == 0\n",
        "\n",
        "# -------- Part 2: train/test features --------\n",
        "rows = []\n",
        "for _, r in df_raw.iterrows():\n",
        "    feats = extract_features(r.get(\"Subject\",\"\"), r.get(\"Message\",\"\"))\n",
        "    rows.append({\n",
        "        \"Message ID\": int(r[\"Message ID\"]),\n",
        "        \"features_json\": json.dumps({k:int(v) for k,v in feats.items()}, separators=(',',':'))\n",
        "    })\n",
        "feat_df = pd.DataFrame(rows, columns=[\"Message ID\",\"features_json\"])\n",
        "train_df = feat_df[~feat_df[\"Message ID\"].apply(is_test)]\n",
        "test_df  = feat_df[ feat_df[\"Message ID\"].apply(is_test)]\n",
        "train_df.to_csv(\"train-features.tsv\", sep=\"\\t\", index=False)\n",
        "test_df.to_csv(\"test-features.tsv\",  sep=\"\\t\", index=False)\n",
        "\n",
        "# -------- Part 3: conditional probabilities (Laplace α=1) --------\n",
        "train_labeled = train_df.merge(df_raw[[\"Message ID\",\"Spam/Ham\"]], on=\"Message ID\", how=\"left\")\n",
        "ALPHA = 1.0\n",
        "counts = {\"ham\": Counter(), \"spam\": Counter()}\n",
        "totals = {\"ham\": 0, \"spam\": 0}\n",
        "vocab = set()\n",
        "for _, row in train_labeled.iterrows():\n",
        "    y = row[\"Spam/Ham\"].strip().lower()\n",
        "    feats = json.loads(row[\"features_json\"])\n",
        "    feats = {str(k): int(v) for k, v in feats.items() if int(v) != 0}\n",
        "    vocab.update(feats.keys())\n",
        "    counts[y].update(feats)\n",
        "    totals[y] += sum(feats.values())\n",
        "V = len(vocab)\n",
        "den_ham  = totals[\"ham\"]  + ALPHA * V\n",
        "den_spam = totals[\"spam\"] + ALPHA * V\n",
        "\n",
        "records = []\n",
        "get_hc = counts[\"ham\"].get\n",
        "get_sc = counts[\"spam\"].get\n",
        "for f in vocab:\n",
        "    p_ham  = (get_hc(f,0) + ALPHA) / den_ham\n",
        "    p_spam = (get_sc(f,0) + ALPHA) / den_spam\n",
        "    records.append((f, p_ham, p_spam))\n",
        "feature_probs = pd.DataFrame(records, columns=[\"feature\",\"ham_probability\",\"spam_probability\"]).sort_values(\"feature\")\n",
        "feature_probs.to_csv(\"feature-probabilities.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "# -------- Parts 4 & 5: Naive Bayes predictions (train & test) --------\n",
        "labels = df_raw[[\"Message ID\",\"Spam/Ham\"]].copy()\n",
        "labels[\"is_spam\"] = (labels[\"Spam/Ham\"].str.lower() == \"spam\").astype(int)\n",
        "labels_train = labels[labels[\"Message ID\"] % 30 != 0]\n",
        "p_spam = labels_train[\"is_spam\"].mean()\n",
        "p_ham  = 1 - p_spam\n",
        "log_p_spam = math.log(max(p_spam, 1e-300))\n",
        "log_p_ham  = math.log(max(p_ham,  1e-300))\n",
        "\n",
        "p_f_ham  = dict(zip(feature_probs[\"feature\"], feature_probs[\"ham_probability\"]))\n",
        "p_f_spam = dict(zip(feature_probs[\"feature\"], feature_probs[\"spam_probability\"]))\n",
        "floor_ham  = max(min((v for v in p_f_ham.values()  if v>0), default=1e-12)*0.1, 1e-12)\n",
        "floor_spam = max(min((v for v in p_f_spam.values() if v>0), default=1e-12)*0.1, 1e-12)\n",
        "from collections import defaultdict\n",
        "log_p_f_ham  = defaultdict(lambda: math.log(floor_ham),  {f: math.log(max(p,1e-300)) for f,p in p_f_ham.items()})\n",
        "log_p_f_spam = defaultdict(lambda: math.log(floor_spam), {f: math.log(max(p,1e-300)) for f,p in p_f_spam.items()})\n",
        "\n",
        "def score(features_json: str):\n",
        "    feats = json.loads(features_json)\n",
        "    ll_s = log_p_spam\n",
        "    ll_h = log_p_ham\n",
        "    for f, c in feats.items():\n",
        "        if c:\n",
        "            ll_s += c * log_p_f_spam[f]\n",
        "            ll_h += c * log_p_f_ham[f]\n",
        "    m = max(ll_s, ll_h)\n",
        "    e_s, e_h = math.exp(ll_s - m), math.exp(ll_h - m)\n",
        "    d = e_s + e_h\n",
        "    return e_h/d, e_s/d\n",
        "\n",
        "def score_df(df_in: pd.DataFrame):\n",
        "    rows = []\n",
        "    for mid, fj in zip(df_in[\"Message ID\"], df_in[\"features_json\"]):\n",
        "        ph, ps = score(fj)\n",
        "        rows.append((mid, ph, ps))\n",
        "    return pd.DataFrame(rows, columns=[\"Message ID\",\"ham\",\"spam\"])\n",
        "\n",
        "train_preds = score_df(train_df); train_preds.to_csv(\"train-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "test_preds  = score_df(test_df);  test_preds.to_csv(\"test-predictions.tsv\",  sep=\"\\t\", index=False)\n",
        "\n",
        "# -------- Part 6: ROC (test only) --------\n",
        "test_labels = labels[labels[\"Message ID\"] % 30 == 0][[\"Message ID\",\"is_spam\"]]\n",
        "dfm = test_labels.merge(test_preds, on=\"Message ID\", how=\"inner\")\n",
        "y_true = dfm[\"is_spam\"].to_numpy()\n",
        "p_sp = dfm[\"spam\"].to_numpy()\n",
        "\n",
        "rows = []\n",
        "ths = np.round(np.arange(0.01, 1.00, 0.01), 2)\n",
        "P = int((y_true==1).sum()); N = int((y_true==0).sum())\n",
        "for t in ths:\n",
        "    yhat = (p_sp >= t).astype(int)\n",
        "    TP = int(((yhat==1)&(y_true==1)).sum())\n",
        "    FP = int(((yhat==1)&(y_true==0)).sum())\n",
        "    tpr = TP / P if P else 0.0\n",
        "    fpr = FP / N if N else 0.0\n",
        "    rows.append((float(t), fpr, tpr))\n",
        "roc = pd.DataFrame(rows, columns=[\"threshold\",\"false_positive_rate\",\"true_positive_rate\"])\n",
        "roc.to_csv(\"roc.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "print(\"Wrote files:\",\n",
        "      *[f\"{p}\" for p in [\"train-features.tsv\",\"test-features.tsv\",\n",
        "                         \"feature-probabilities.tsv\",\"train-predictions.tsv\",\n",
        "                         \"test-predictions.tsv\",\"roc.tsv\"]], sep=\"\\n - \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'acknowledgments.txt and acknowledgments.tsv written.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create TSV-formatted acknowledgments file(s)\n",
        "content = (\n",
        "    \"Name\\tAffiliation\\tContribution\\tLink\\n\"\n",
        "    \"ChatGPT (OpenAI)\\tGenerative AI assistant\\tHelped with feature design, Naive Bayes math, debugging, and ROC code for Week 9.\\tN/A\\n\"\n",
        "    \"MWiechmann/enron_spam_data\\tGitHub repository\\tData source zip referenced in instructions.\\thttps://github.com/MWiechmann/enron_spam_data\\n\"\n",
        "    \"bu-cds-dx704/dx704-project-09\\tGitHub repository\\tProject description and template notebook.\\thttps://github.com/bu-cds-dx704/dx704-project-09\\n\"\n",
        "    \"bu-cds-omds example repos\\tGitHub repositories\\tExample notebooks used for reference.\\thttps://github.com/bu-cds-omds\\n\"\n",
        ")\n",
        "\n",
        "# Save as acknowledgments.txt (expected by autograder) and also acknowledgments.tsv for convenience\n",
        "with open(\"acknowledgments.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "with open(\"acknowledgments.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "\"acknowledgments.txt and acknowledgments.tsv written.\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
